# Weather Data Quality Platform ğŸŒ¤ï¸

A production data engineering pipeline that ingests, transforms, and monitors weather data quality using modern cloud-native tools.

## ğŸ¯ Project Goals

- Build end-to-end data pipeline from ingestion to visualization
- Implement comprehensive data quality framework
- Demonstrate infrastructure-as-code best practices
- Showcase skills relevant to Big Tech and Middle East tech companies

## ğŸ—ï¸ Architecture

```
Open-Meteo API â†’ AWS Lambda â†’ S3 (raw) â†’ dbt â†’ S3 (staging/mart) 
â†’ Great Expectations â†’ Athena â†’ Streamlit Dashboard
```

**Orchestration:** Apache Airflow  
**Infrastructure:** Terraform  
**CI/CD:** GitHub Actions

## ğŸ“Š Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| Cloud | AWS | Free tier, MENA region presence |
| Storage | S3 + Athena | Serverless data lake |
| Compute | Lambda | Event-driven ingestion |
| Transformation | dbt | SQL-based analytics engineering |
| Data Quality | Great Expectations | Automated validation & profiling |
| Orchestration | Airflow | Workflow management |
| IaC | Terraform | Infrastructure provisioning |
| CI/CD | GitHub Actions | Automated testing & deployment |

## ğŸš€ Quick Start

### Local Development

```bash
# Clone repo
git clone https://github.com/andikabhaskara/weather-dq-platform.git
cd weather-dq-platform

# Setup environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run ingestion
python src/ingestion.py
```

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ src/               # Source code
â”œâ”€â”€ terraform/         # Infrastructure as code
â”œâ”€â”€ dbt/              # Data transformation models
â”œâ”€â”€ tests/            # Unit & integration tests
â””â”€â”€ docs/             # Documentation & architecture diagrams
```

## âœ… Current Status

**Phase 1A: Local Ingestion** âœ… Complete
- Multi-city weather data ingestion (New York, Singapore, Tokyo)
- Pydantic-based schema validation
- Retry logic with exponential backoff
- Structured logging

**Phase 1B: AWS Deployment** ğŸš§ In Progress

## ğŸ“ˆ Data Contract

- **Sources:** Open-Meteo Historical Weather API
- **Locations:** 3 cities (NY, Singapore, Tokyo)
- **Frequency:** Daily backfill (30-day rolling window)
- **Volume:** ~2,160 records/day
- **SLA:** Data ingested within 24 hours

See [DATA_CONTRACT.md](DATA_CONTRACT.md) for full schema & validation rules.

## ğŸ“ Learning Outcomes

- âœ… API integration with error handling
- âœ… Data validation using Pydantic
- âœ… Logging best practices
- ğŸš§ AWS Lambda deployment
- ğŸš§ dbt transformation patterns
- ğŸš§ Data quality monitoring

## ğŸ“§ Contact

Built by Andika Bhaskara as part of portfolio for Data Platform Engineer roles.

[LinkedIn](#) | [Blog Post](#)


========================================



# weather-data-quality-platform

Goal: Monitor weather data quality for 10 ASEAN cities

Architecture diagram (simple boxes/arrows)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Open-Meteo API â”‚ (External data source)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1. Python script fetches weather data every hour
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GCP Pub/Sub   â”‚ (Message queue - buffers data)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 2. Pub/Sub triggers function to load data
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BigQuery (raw) â”‚ (Data warehouse - stores raw JSON)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 3. dbt transforms raw â†’ clean tables
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BigQuery (mart) â”‚ (Clean, modeled data)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 4. Python runs data quality checks
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DQ Results     â”‚ (Pass/Fail metrics stored in BigQuery)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 5. Airflow orchestrates steps 1-4 daily
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Dashboard    â”‚ (Streamlit - visualizes DQ metrics)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Infrastructure: Terraform provisions all GCP resources
CI/CD: GitHub Actions deploys code changes

Input: Fetch weather for 10 cities (Jakarta, Singapore, Tokyo, etc.) every hour
Storage: Save raw JSON to BigQuery
Transform (dbt): Create clean tables (staging â†’ mart)
Quality checks:
Are temperatures within valid range (-50Â°C to 60Â°C)?
Is any data missing?
Does Singapore temp match expected pattern?
Orchestrate: Airflow runs this daily
Visualize: Streamlit dashboard shows "95% data passed quality checks"

Tech stack: Python, GCP Pub/Sub, BigQuery, dbt, Airflow, Terraform

